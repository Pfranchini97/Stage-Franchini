{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5291,"status":"ok","timestamp":1682610900819,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"6qGq8OeftJhc"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import os"]},{"cell_type":"markdown","source":["This is the code fot the simple model that uses Mnist"],"metadata":{"id":"i0q-UzDCvoMH"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"3__s4949tOVd","executionInfo":{"status":"ok","timestamp":1682610900820,"user_tz":-120,"elapsed":7,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"}}},"outputs":[],"source":["(ds_train, ds_test), ds_info = tfds.load(\n","    'mnist',\n","    split = ['train', 'test'],\n","    shuffle_files=True,\n","    as_supervised=True,\n","    with_info=True,\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682610900820,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"ATJm1hd1y4SW"},"outputs":[],"source":["#ds_train = train_ds_gen\n","#ds_test = train_ds_gen"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682610900820,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"XqtVHaqOoNxU"},"outputs":[],"source":["def normalize_img(image, label):\n","  return tf.cast(image, tf.float32) / 255., label"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"NLlIVaX2ohYa","executionInfo":{"status":"ok","timestamp":1682610900820,"user_tz":-120,"elapsed":3,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"}}},"outputs":[],"source":["ds_train = ds_train.map(\n","    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n","ds_train = ds_train.cache()\n","ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n","ds_train = ds_train.batch(128)\n","ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"e3Uz8d4OyB4A","executionInfo":{"status":"ok","timestamp":1682610901183,"user_tz":-120,"elapsed":366,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"}}},"outputs":[],"source":["ds_test = ds_test.map(\n","    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n","ds_test = ds_test.batch(128)\n","ds_test = ds_test.cache()\n","ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35733,"status":"ok","timestamp":1682610936900,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"Gn6lK6RayiTP","outputId":"e2c6b055-2cb4-479e-9e6f-ba4487590824"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/6\n","469/469 [==============================] - 17s 12ms/step - loss: 0.3545 - sparse_categorical_accuracy: 0.9036 - val_loss: 0.1970 - val_sparse_categorical_accuracy: 0.9419\n","Epoch 2/6\n","469/469 [==============================] - 2s 5ms/step - loss: 0.1657 - sparse_categorical_accuracy: 0.9524 - val_loss: 0.1480 - val_sparse_categorical_accuracy: 0.9548\n","Epoch 3/6\n","469/469 [==============================] - 2s 5ms/step - loss: 0.1195 - sparse_categorical_accuracy: 0.9662 - val_loss: 0.1116 - val_sparse_categorical_accuracy: 0.9656\n","Epoch 4/6\n","469/469 [==============================] - 2s 5ms/step - loss: 0.0921 - sparse_categorical_accuracy: 0.9735 - val_loss: 0.0961 - val_sparse_categorical_accuracy: 0.9713\n","Epoch 5/6\n","469/469 [==============================] - 2s 5ms/step - loss: 0.0739 - sparse_categorical_accuracy: 0.9784 - val_loss: 0.0831 - val_sparse_categorical_accuracy: 0.9748\n","Epoch 6/6\n","469/469 [==============================] - 4s 7ms/step - loss: 0.0612 - sparse_categorical_accuracy: 0.9826 - val_loss: 0.0842 - val_sparse_categorical_accuracy: 0.9745\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fa4f02b3550>"]},"metadata":{},"execution_count":7}],"source":["model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10)\n","])\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(0.001),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",")\n","\n","model.fit(\n","    ds_train,\n","    epochs=6,\n","    validation_data = ds_test\n",")"]},{"cell_type":"markdown","source":["From here there is my attempt at creating my datagenerator. First I save the files in a directory called 'data'."],"metadata":{"id":"qsc1Ow1dv5bC"}},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":251,"status":"ok","timestamp":1682610965666,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"-PLUTqNXTpmf"},"outputs":[],"source":["#os.mkdir('data')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682610966027,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"tX0S-Q-k88rm"},"outputs":[],"source":["(mnist_train, mnist_test)= tfds.load(\n","    'mnist',\n","    split = ['train', 'test'],\n","    shuffle_files=True,\n","    as_supervised=True\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682610966027,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"fHMNseWY9Zd8"},"outputs":[],"source":["iter_train = mnist_train.as_numpy_iterator()\n","iter_test = mnist_test.as_numpy_iterator()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1682610966260,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"vgPP3v0N9anU"},"outputs":[],"source":["lista_train = []\n","lista_test = []"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58855,"status":"ok","timestamp":1682611025113,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"FcS3UGYiNp4h","outputId":"cd5da9ea-23d5-459c-ef33-cf93b36e54d0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  arr = np.asanyarray(arr)\n"]}],"source":["#I use np.save, but there probably are some problems with the format of files\n","#as I get a Warning of Visible Deprecation. I tried solving it by allowing pickle\n","\n","offset = 0\n","for i, el in enumerate(iter_train):\n","  np.save('data/arr'+str(i), el, allow_pickle=True)\n","  lista_train.append('arr'+str(i))\n","  offset = i + 1\n","for i, el in enumerate(iter_test):\n","  np.save('data/arr'+str(i + offset), el, allow_pickle=True)\n","  lista_test.append('arr'+str(i + offset))\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682611025113,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"u-nCumGkaCr1","outputId":"38384263-bb13-4a25-a1d4-12480d268bd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["arr59999\n","arr60000\n"]}],"source":["#just checking that the lists are correct\n","print(lista_train[-1])\n","print(lista_test[0])"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":21834,"status":"ok","timestamp":1682611046945,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"417WvihEuEnr"},"outputs":[],"source":["labels_train = {e : np.load('data/' + e + '.npy', allow_pickle = True)[1] for e in lista_train}\n","labels_test = {e : np.load('data/' + e + '.npy', allow_pickle = True)[1] for e in lista_test}"]},{"cell_type":"code","source":["labels_test['arr60000']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nMS15TTW_ZR","executionInfo":{"status":"ok","timestamp":1682611046946,"user_tz":-120,"elapsed":15,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"}},"outputId":"2d7957a1-4a85-4160-d33c-721211ce8613"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682611046946,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"s2oGpGkpbCIU"},"outputs":[],"source":["partition = {'train' : lista_train, 'test' : lista_test}"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682611046947,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"9QIcrhpkbKrU"},"outputs":[],"source":["#This is the code for the custom datagenerator, that should go in a separate script\n","\n","class DataGenerator(keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n","                 n_classes=10, shuffle=True):\n","        'Initialization'\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.labels = labels\n","        self.list_IDs = list_IDs\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(list_IDs_temp)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n","        y = np.empty((self.batch_size), dtype=int)\n","\n","        # Generate data\n","        for i, ID in enumerate(list_IDs_temp):\n","            # Store sample\n","            X[i,] = np.load('data/' + ID + '.npy', allow_pickle = True)[0]\n","\n","            # Store class\n","            y[i] = self.labels[ID]\n","\n","        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682611075156,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"BvG_DoWqo-BF"},"outputs":[],"source":["params = {'dim': (28, 28),\n","          'batch_size': 128,\n","          'n_classes': 10,\n","          'n_channels': 1,\n","          'shuffle': True}"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682611075156,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"GwYEvdGVxxJ4"},"outputs":[],"source":["train_ds_gen = DataGenerator(partition['train'], labels_train, **params)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682611075157,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"E9rP7vbRyVuB"},"outputs":[],"source":["test_ds_gen = DataGenerator(partition['test'], labels_test, **params)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2140,"status":"error","timestamp":1682611078944,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"},"user_tz":-120},"id":"72sNdGXFyv5j","outputId":"09c9b229-6c32-4e1e-ef00-b9c810437127"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/6\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-70cf2fbb8484>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_ds_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'Squeeze' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-30-70cf2fbb8484>\", line 12, in <cell line: 12>\n      model.fit(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/metrics/base_metric.py\", line 691, in update_state\n      matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/metrics_utils.py\", line 963, in sparse_categorical_matches\n      y_true = tf.squeeze(y_true, [-1])\nNode: 'Squeeze'\nCan not squeeze dim[1], expected a dimension of 1, got 10\n\t [[{{node Squeeze}}]] [Op:__inference_train_function_155505]"]}],"source":["model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10)\n","])\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(0.001),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",")\n","\n","model.fit(\n","    train_ds_gen,\n","    epochs=6,\n","    validation_data = test_ds_gen\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7k6ziXze70-W","executionInfo":{"status":"aborted","timestamp":1682611046948,"user_tz":-120,"elapsed":14,"user":{"displayName":"Aster Stear","userId":"08717517503324891683"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMOvDqjWg8Yercs6W8AE8TO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}